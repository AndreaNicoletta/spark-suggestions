<html>
<body>
  <head>
    <link rel="stylesheet" href="style.css">
    <title> Spark suggestions</title>
  </head>

<h1>Table of Contents</h1>

<ul>
  <li><a href="#intro">Introduction</a></li>
  <li><a href="#table1">Applications properties</a></li>
  <li><a href="#table2">Runtime environment</a></li>
  <li><a href="#table3">Table 3</a></li>
  <li><a href="#biblio">Bibliography</a></li>
</ul>

<h2 id="intro">Introduction</h2>
"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? Quis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, vel illum qui dolorem eum fugiat quo voluptas nulla pariatur?" id est laborum.Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? Quis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, vel illum qui dolorem eum fugiat quo voluptas nulla pariatur?"
<br><br>
<h2 id="table1">Applications properties</h2>

<br><br>
<table>
  <tr>
    <th>Property Name</th>
    <th>Meaning</th>
    <th>Default</th>
    <th>Suggested</th>
  </tr>

  <! app name>
  <tr >
    <td>spark.app.name</td>  
    <td>name da dare all'applicazione/sessione spark</td>
    <td>(none)</td>
    <td>nome parlante utile per consultare la UI</td>
  </tr>

  <tr>
    <td>spark.driver.cores</td>
    <td>Number of cores to use for the driver process, only in cluster mode.</td> 
    <td>1</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.driver.maxResultSize</td>
    <td>  Limit of total size of serialized results of all partitions for each Spark action (e.g. collect) in bytes. 
      Should be at least 1M, or 0 for unlimited. Jobs will be aborted if the total size is above this limit. 
      Having a high limit may cause out-of-memory errors in driver (depends on spark.driver.memory and memory overhead 
      of objects in JVM). Setting a proper limit can protect the driver from out-of-memory errors.</td> 
    <td>1g</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.driver.memory</td>
    <td>  	Amount of memory to use for the driver process, i.e. where SparkContext is initialized, in the same format as JVM memory strings with a size unit suffix ("k", "m", "g" or "t") (e.g. 512m, 2g).
      Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-memory command line option or in your default properties file.</td> 
    <td>1g</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.driver.memoryOverhead</td>
    <td>Amount of non-heap memory to be allocated per driver process in cluster mode, in MiB unless otherwise specified. 
      This is memory that accounts for things like
       VM overheads, interned strings, other native overheads, etc. This tends to grow with 
       the container size (typically 6-10%). This option is currently supported on YARN,
        Mesos and Kubernetes. Note: Non-heap memory includes off-heap memory 
        (when spark.memory.offHeap.enabled=true) and memory used by other driver 
        processes (e.g. python process that goes with a PySpark driver) and memory 
        used by other non-driver processes running in the same container. 
        The maximum memory size of container to running driver is determined
         by the sum of spark.driver.memoryOverhead and spark.driver.memory.</td> 
    <td>driverMemory * spark.driver.memoryOverheadFactor, with minimum of 384</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.driver.memoryOverheadFactor</td>
    <td>Fraction of driver memory to be allocated as additional 
      non-heap memory per driver process in cluster mode. 
      This is memory that accounts for things like VM overheads, interned strings,
       other native overheads, etc. This tends to grow with the container size. 
       This value defaults to 0.10 except for Kubernetes non-JVM jobs, which defaults to 0.40. 
       This is done as non-JVM tasks need more non-JVM heap space and such tasks commonly 
       fail with "Memory Overhead Exceeded" errors. This preempts this error with a higher default. 
       This value is ignored if spark.driver.memoryOverhead is set directly.	3.3.0
    </td> 
    <td>0.10</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.executor.resource.{resourceName}.amount</td>
    <td>Amount of a particular resource type to use per executor process. 
      If this is used, you must also specify the spark.executor.resource.{resourceName}.discoveryScript 
      for the executor to find the resource on startup.	3.0.0
    </td> 
    <td>0</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.executor.resource.{resourceName}.discoveryScript</td>
    <td>A script for the executor to run to discover a particular resource type. 
      This should write to STDOUT a JSON string in the format of the ResourceInformation class. 
      This has a name and an array of addresses.
    </td> 
    <td>None</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.executor.resource.{resourceName}.vendor</td>
    <td>Vendor of the resources to use for the executors.
       This option is currently only supported on Kubernetes and is actually 
       both the vendor and domain following the Kubernetes device plugin naming 
       convention. (e.g. For GPUs on Kubernetes this config would be set to nvidia.com or amd.com)
    </td> 
    <td>None</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.extraListeners</td>
    <td>A comma-separated list of classes that implement SparkListener; 
      when initializing SparkContext, instances of these classes will be created 
      and registered with Spark's listener bus. If a class has a single-argument 
      constructor that accepts a SparkConf, that constructor will be called; otherwise, 
      a zero-argument constructor will be called. If no valid constructor can be found,
       the SparkContext creation will fail with an exception.	1.3.0
    </td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.local.dir</td>
    <td>Directory to use for "scratch" space in Spark, including map output files and RDDs 
      that get stored on disk. This should be on a fast, local disk in your system. 
      It can also be a comma-separated list of multiple directories on different disks.
      Note: This will be overridden by SPARK_LOCAL_DIRS (Standalone),
       MESOS_SANDBOX (Mesos) or LOCAL_DIRS (YARN) environment variables 
       set by the cluster manager.	0.5.0
      
    </td> 
    <td>/tmp</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.logConf</td>
    <td>	Logs the effective SparkConf as INFO when a SparkContext is started.
    </td> 
    <td>false</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.master</td>
    <td>The cluster manager to connect to. See the list of allowed master URL's.
    </td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.submit.deployMode</td>
    <td>The deploy mode of Spark driver program, either "client" or "cluster", 
      Which means to launch driver program locally ("client") or remotely ("cluster") 
      on one of the nodes inside the cluster.	1.5.0

    </td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.log.callerContext</td>
    <td>Application information that will be written into Yarn RM log/HDFS
      audit log when running on Yarn/HDFS. Its length depends on the Hadoop 
      configuration hadoop.caller.context.max.size. It should be concise,
       and typically can have up to 50 characters.	2.2.0

    </td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.log.level</td>
    <td>When set, overrides any user-defined log settings as 
      if calling SparkContext.setLogLevel() at Spark startup. 
      Valid log levels include: "ALL", "DEBUG", "ERROR", "FATAL", "INFO", "OFF", "TRACE", "WARN".
    </td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.driver.supervise</td>
    <td>If true, restarts the driver automatically if it fails with a non-zero exit status. 
      Only has effect in Spark standalone mode or Mesos cluster deploy mode
    </td> 
    <td>false</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.driver.log.dfsDir</td>
    <td>Base directory in which Spark driver logs are synced, 
      if spark.driver.log.persistToDfs.enabled is true. Within this base directory, 
      each application logs the driver logs to an application specific file.
       Users may want to set this to a unified location like an HDFS directory so driver
        log files can be persisted for later usage. This directory should allow 
        any Spark user to read/write files and the Spark History Server user to delete files. 
        Additionally, older logs from this directory are cleaned by the Spark History Server 
        if spark.history.fs.driverlog.cleaner.enabled is true and, if they are older 
        than max age configured by setting spark.history.fs.driverlog.cleaner.maxAge.	3.0.0

    </td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.driver.log.persistToDfs.enabled</td>
    <td>If true, spark application running in client mode will write driver logs to a persistent storage, 
      configured in spark.driver.log.dfsDir. If spark.driver.log.dfsDir is not configured, 
      driver logs will not be persisted. Additionally, enable the cleaner
       by setting spark.history.fs.driverlog.cleaner.enabled to true in Spark History Server.
    </td> 
    <td>false</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.driver.log.layout</td>
    <td>The layout for the driver logs that are synced to spark.driver.log.dfsDir. 
      If this is not configured, it uses the layout for the first appender defined in log4j2.properties.
       If that is also not configured, driver logs use the default layout.	3.0.0

    </td> 
    <td>%d{yy/MM/dd HH:mm:ss.SSS} %t %p %c{1}: %m%n%ex</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.driver.log.allowErasureCoding</td>
    <td>Whether to allow driver logs to use erasure coding. 
      On HDFS, erasure coded files will not update as quickly as regular replicated files, 
      so they make take longer to reflect changes written by the application.
       Note that even if this is true, Spark will still not force the file to use erasure coding, 
       it will simply use file system defaults.
    </td> 
    <td>false</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.decommission.enabled</td>
    <td>When decommission enabled, Spark will try its best to shut down the executor gracefully. 
      Spark will try to migrate all the RDD blocks (controlled by spark.storage.decommission.rddBlocks.enabled) 
      and shuffle blocks (controlled by spark.storage.decommission.shuffleBlocks.enabled) 
      from the decommissioning executor to a remote executor when spark.storage.decommission.enabled is enabled.
       With decommission enabled, Spark will also decommission an executor instead of killing when
    </td> 
    <td>false</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.executor.decommission.killInterval</td>
    <td>Duration after which a decommissioned executor will be killed forcefully 
      by an outside (e.g. non-spark) service.	3.1.0
    </td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.executor.decommission.forceKillTimeout</td>
    <td>Duration after which a Spark will force a decommissioning executor to exit. 
      This should be set to a high value in most situations as low values will prevent
       block migrations from having enough time to complete.
    </td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

  <tr>
    <td>spark.executor.decommission.signal</td>
    <td>The signal that used to trigger the executor to start decommission.
    </td> 
    <td>PWR</td> 
    <td>Inserire valore e inserire spiegazione: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.<cite><a href="#uno">[1]</a></cite></td>
  </tr>

</table>
<br><br>
  "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? Quis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, vel illum qui dolorem eum fugiat quo voluptas nulla pariatur?"

<h2 id="table2">Runtime environment</h2> 


<br><br>
<table>
  <tr>
    <th>Property Name</th>
    <th>Meaning</th>
    <th>Default</th>
    <th>Suggested</th>
  </tr>

  <! app name>
  <tr >
    <td>spark.driver.extraClassPath</td>  
    <td>	Extra classpath entries to prepend to the classpath of the driver.
      Note: In client mode, this config must not be set through the SparkConf directly in your application, 
      because the driver JVM has already started at that point. 
      Instead, please set this through the --driver-class-path command line option or in your default properties file.</td>
    <td>(none)</td>
    <td>nome parlante utile per consultare la UI</td>
  </tr>

  <tr>
    <td>spark.driver.defaultJavaOptions</td>
    <td>A string of default JVM options to prepend to spark.driver.extraJavaOptions. This is intended to be set by administrators. 
      For instance, GC settings or other logging. Note that it is illegal to set maximum heap size (-Xmx) settings with this option. 
      Maximum heap size settings can be set with spark.driver.memory in the cluster mode and through the 
      --driver-memory command line option in the client mode.
      Note: In client mode, this config must not be set through the SparkConf directly in your application, 
      because the driver JVM has already started at that point. Instead, please set this through the --driver-java-options command 
      line option or in your default properties file.</td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.driver.extraJavaOptions</td>
    <td>A string of extra JVM options to pass to the driver. This is intended to be set by users. 
      For instance, GC settings or other logging. Note that it is illegal to set maximum heap size (-Xmx) settings with this option.
       Maximum heap size settings can be set with spark.driver.memory in the cluster mode and through the --driver-memory command line option in the client mode.
      Note: In client mode, this config must not be set through the SparkConf directly in your application, because the 
      driver JVM has already started at that point. Instead, please set this through the --driver-java-options command line option
       or in your default properties file. spark.driver.defaultJavaOptions will be prepended to this configuration.</td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.driver.extraLibraryPath</td>
    <td>Set a special library path to use when launching the driver JVM.
      Note: In client mode, this config must not be set through the SparkConf directly in your application, 
      because the driver JVM has already started at that point. 
      Instead, please set this through the --driver-library-path command line option or in your default properties file.</td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.driver.userClassPathFirst</td>
    <td>(Experimental) Whether to give user-added jars precedence over Spark's own jars when 
      loading classes in the driver. This feature can be used to mitigate conflicts between 
      Spark's dependencies and user dependencies. It is currently an experimental feature. 
      This is used in cluster mode only.</td> 
    <td>false</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.executor.extraClassPath</td>
    <td>Extra classpath entries to prepend to the classpath of executors.
       This exists primarily for backwards-compatibility with older versions of Spark.
        Users typically should not need to set this option.</td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.executor.defaultJavaOptions</td>
    <td>A string of default JVM options to prepend to spark.executor.extraJavaOptions. 
      This is intended to be set by administrators. For instance, GC settings or other logging. 
      Note that it is illegal to set Spark properties or maximum heap size (-Xmx) settings with this option.
       Spark properties should be set using a SparkConf object or the spark-defaults.conf 
       file used with the spark-submit script. Maximum heap size settings can be set with spark.executor.memory. 
       The following symbols, if present will be interpolated: will be replaced by application ID and will 
       be replaced by executor ID. For example, to enable verbose gc logging to a file named for 
       the executor ID of the app in /tmp, pass a 'value' of: -verbose:gc -Xloggc:/tmp/-.gc</td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.executor.extraJavaOptions</td>
    <td>A string of extra JVM options to pass to executors. This is intended to be set by users.
       For instance, GC settings or other logging. Note that it is illegal to set Spark properties 
       or maximum heap size (-Xmx) settings with this option. Spark properties should be set 
       using a SparkConf object or the spark-defaults.conf file used with the spark-submit script.
        Maximum heap size settings can be set with spark.executor.memory. The following symbols, 
        if present will be interpolated: will be replaced by application ID and will be replaced 
        by executor ID. For example, to enable verbose gc logging to a file named for the executor 
        ID of the app in /tmp, pass a 'value' of: -verbose:gc -Xloggc:/tmp/-.gc spark.executor.defaultJavaOptions 
        will be prepended to this configuration.</td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.executor.extraLibraryPath</td>
    <td>Set a special library path to use when launching executor JVM's.</td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.executor.logs.rolling.maxRetainedFiles</td>
    <td>Sets the number of latest rolling log files that are going to be retained by the system.
       Older log files will be deleted. Disabled by default.</td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.executor.logs.rolling.enableCompression</td>
    <td>Enable executor log compression. If it is enabled, 
      the rolled executor logs will be compressed. Disabled by default.</td> 
    <td>false</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.executor.logs.rolling.maxSize</td>
    <td>Set the max size of the file in bytes by which the executor logs will be rolled over. 
      Rolling is disabled by default. 
      See spark.executor.logs.rolling.maxRetainedFiles for automatic cleaning of old logs.</td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.executor.logs.rolling.strategy</td>
    <td>Set the strategy of rolling of executor logs. 
      By default it is disabled. It can be set to "time" 
      (time-based rolling) or "size" (size-based rolling). 
      For "time", use spark.executor.logs.rolling.time.interval
       to set the rolling interval. For "size", 
       use spark.executor.logs.rolling.maxSize to set the 
       maximum file size for rolling.</td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.executor.logs.rolling.time.interval</td>
    <td>Set the time interval by which the executor logs will be 
      rolled over. Rolling is disabled by default. Valid values 
      are daily, hourly, minutely or any interval in seconds.
       See spark.executor.logs.rolling.maxRetainedFiles 
       for automatic cleaning of old logs.</td> 
    <td>daily</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.executor.userClassPathFirst</td>
    <td>(Experimental) Same functionality as 
      spark.driver.userClassPathFirst, 
      but applied to executor instances. </td> 
    <td>false</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.executorEnv.[EnvironmentVariableName]</td>
    <td>Add the environment variable specified 
      by EnvironmentVariableName to the Executor process. 
      The user can specify multiple of these to set multiple 
      environment variables. </td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.redaction.regex</td>
    <td>Regex to decide which Spark configuration properties and 
      environment variables in driver and executor environments
       contain sensitive information. When this regex matches a 
       property key or value, the value is redacted from the 
       environment UI and various logs like YARN and event logs.</td> 
    <td>(?i)secret|password|token|access[.]key</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.redaction.string.regex</td>
    <td>Regex to decide which parts of strings produced by 
      Spark contain sensitive information. When this regex matches a string part, 
      that string part is replaced by a dummy value. 
      This is currently used to redact the output of SQL explain commands.</td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.python.profile</td>
    <td>Enable profiling in Python worker, the profile result will 
      show up by sc.show_profiles(), or it will be displayed before 
      the driver exits. It also can be dumped into disk by 
      sc.dump_profiles(path). If some of the profile results had
       been displayed manually, they will not be displayed 
       automatically before driver exiting. By default the 
       pyspark.profiler.BasicProfiler will be used, but this can 
       be overridden by passing a profiler class in as a parameter 
       to the SparkContext constructor.</td> 
    <td>false</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.python.profile.dump</td>
    <td>The directory which is used to dump the profile result 
      before driver exiting. The results will be dumped as 
      separated file for each RDD. They can be loaded
       by pstats.Stats(). If this is specified, 
       the profile result will not be displayed automatically</td> 
    <td>(none)</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.python.worker.memory</td>
    <td>Amount of memory to use per python worker process
       during aggregation, in the same format as
        JVM memory strings with a size unit suffix 
        ("k", "m", "g" or "t") (e.g. 512m, 2g). 
        If the memory used during aggregation goes 
        above this amount, it will spill the data into disks.</td> 
    <td>512m</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.python.worker.reuse</td>
    <td>Reuse Python worker or not. 
      If yes, it will use a fixed number of Python workers,
       does not need to fork() a Python process for every task. 
       It will be very useful if there is a large broadcast, 
       then the broadcast will not need to be transferred from 
       JVM to Python worker for every task.</td> 
    <td>true</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.files</td>
    <td>Comma-separated list of files to be placed in the 
      working directory of each executor. Globs are allowed.</td> 
    <td></td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.submit.pyFiles </td>
    <td>Comma-separated list of .zip, .egg, or .py files 
      to place on the PYTHONPATH for Python apps. Globs are allowed.</td> 
    <td></td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.jars</td>
    <td>Comma-separated list of jars to include on the driver 
      and executor classpaths. Globs are allowed.</td> 
    <td></td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.jars.packages</td>
    <td>Comma-separated list of Maven coordinates of jars to include 
      on the driver and executor classpaths. The coordinates
       should be groupId:artifactId:version.
        If spark.jars.ivySettings is given artifacts 
        will be resolved according to the configuration 
        in the file, otherwise artifacts will be searched 
        for in the local maven repo, then maven central 
        and finally any additional remote repositories given 
        by the command-line option --repositories. 
        For more details, see Advanced Dependency Management.</td> 
    <td></td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.jars.excludes</td>
    <td>Comma-separated list of groupId:artifactId,
       to exclude while resolving the dependencies 
      provided in spark.jars.packages to avoid dependency conflicts.</td> 
    <td></td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.jars.ivy</td>
    <td>Path to specify the Ivy user directory,
       used for the local Ivy cache and package files
        from spark.jars.packages. This will override the
         Ivy property ivy.default.ivy.user.dir 
         which defaults to ~/.ivy2.</td> 
    <td></td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.jars.ivySettings</td>
    <td>Path to an Ivy settings file to customize resolution 
      of jars specified using spark.jars.packages instead of 
      the built-in defaults, such as maven central. Additional
       repositories given by the command-line option --repositories 
       or spark.jars.repositories will also be included. Useful 
       for allowing Spark to resolve artifacts from behind a 
       firewall e.g. via an in-house artifact server like Artifactory.
        Details on the settings file format can be found at
         Settings Files. Only paths with file:// scheme are 
         supported. Paths without a scheme are assumed to have 
         a file:// scheme.
      When running in YARN cluster mode, this file will also be
       localized to the remote driver for dependency resolution
        within SparkContext#addJar</td> 
    <td></td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.jars.repositories</td>
    <td>Comma-separated list of additional remote repositories
       to search for the maven coordinates given with --packages
        or spark.jars.packages.</td> 
    <td></td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.archives</td>
    <td>Comma-separated list of archives to be extracted into 
      the working directory of each executor. .jar, .tar.gz, .tgz
       and .zip are supported. You can specify the directory name 
       to unpack via adding # after the file name to unpack, 
       for example, file.zip#directory. This configuration
        is experimental.</td> 
    <td></td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.pyspark.driver.python</td>
    <td>  Python binary executable to use for PySpark in driver.</td> 
    <td>(spark.pyspark.python)</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>

  <tr>
    <td>spark.pyspark.python </td>
    <td>Python binary executable to use for PySpark 
      in both driver and executors.</td> 
    <td></td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>


</table>
<br><br>


<h2 id="table3">Table 3</h2>


<br><br>
<table>
  <tr>
    <th>Property Name</th>
    <th>Meaning</th>
    <th>Default</th>
    <th>Suggested</th>
  </tr>

  <! app name>
  <tr >
    <td>spark.app.name</td>  
    <td>name da dare all'applicazione/sessione spark</td>
    <td>(none)</td>
    <td>nome parlante utile per consultare la UI</td>
  </tr>

  <tr>
    <td>spark.driver.cores</td>
    <td>Number of cores to use for the driver process, only in cluster mode.</td> 
    <td>1</td> 
    <td>Inserire valore e inserire spiegazione<cite><a href="#dodici">[12]</a></cite></td>
  </tr>
</table>
<br><br>



<section id="bibliogr">
  <h2 id="biblio">Bibliography</h2>
  

  <div class="reference">
    <span class="reference-type"><a id="uno">[1]</a></span>
    <span class="reference-type">Book</span>
    <span class="reference-details">
      <span class="reference-author">Author Lastname, Firstname</span>,  
      <span class="reference-year">(2019)</span>. Book Title. Publisher.
    </span>
  </div>

  <div class="reference">
    <span class="reference-type"><a id="due">[2]</a></span>
    <span class="reference-type">Book</span>
    <span class="reference-details">
      <span class="reference-author">Author Lastname, Firstname</span>,  
      <span class="reference-year">(2019)</span>. Book Title. Publisher.
    </span>
  </div>

  <div class="reference">
    <span class="reference-type"><a id="tre">[3]</a></span>
    <span class="reference-type">Book</span>
    <span class="reference-details">
      <span class="reference-author">Author Lastname, Firstname</span>,  
      <span class="reference-year">(2019)</span>. Book Title. Publisher.
    </span>
  </div>

  <div class="reference">
    <span class="reference-type"><a id="quattro">[4]</a></span>
    <span class="reference-type">Book</span>
    <span class="reference-details">
      <span class="reference-author">Author Lastname, Firstname</span>,  
      <span class="reference-year">(2019)</span>. Book Title. Publisher.
    </span>
  </div>

  <div class="reference">
    <span class="reference-type"><a id="cinque">[5]</a></span>
    <span class="reference-type">Book</span>
    <span class="reference-details">
      <span class="reference-author">Author Lastname, Firstname</span>,  
      <span class="reference-year">(2019)</span>. Book Title. Publisher.
    </span>
  </div>

  <div class="reference">
    <span class="reference-type"><a id="sei">[6]</a></span>
    <span class="reference-type">Book</span>
    <span class="reference-details">
      <span class="reference-author">Author Lastname, Firstname</span>,  
      <span class="reference-year">(2019)</span>. Book Title. Publisher.
    </span>
  </div>

  <div class="reference">
    <span class="reference-type"><a id="sette">[7]</a></span>
    <span class="reference-type">Book</span>
    <span class="reference-details">
      <span class="reference-author">Author Lastname, Firstname</span>,  
      <span class="reference-year">(2019)</span>. Book Title. Publisher.
    </span>
  </div>

  <div class="reference">
    <span class="reference-type"><a id="otto">[8]</a></span>
    <span class="reference-type">Book</span>
    <span class="reference-details">
      <span class="reference-author">Author Lastname, Firstname</span>,  
      <span class="reference-year">(2019)</span>. Book Title. Publisher.
    </span>
  </div>

  <div class="reference">
    <span class="reference-type"><a id="nove">[9]</a></span>
    <span class="reference-type">Book</span>
    <span class="reference-details">
      <span class="reference-author">Author Lastname, Firstname</span>,  
      <span class="reference-year">(2019)</span>. Book Title. Publisher.
    </span>
  </div>

  <div class="reference">
    <span class="reference-type"><a id="dieci">[10]</a></span>
    <span class="reference-type">Book</span>
    <span class="reference-details">
      <span class="reference-author">Author Lastname, Firstname</span>,  
      <span class="reference-year">(2019)</span>. Book Title. Publisher.
    </span>
  </div>

  <div class="reference">
    <span class="reference-type"><a id="undici">[11]</a></span>
    <span class="reference-type">Book</span>
    <span class="reference-details">
      <span class="reference-author">Author Lastname, Firstname</span>,  
      <span class="reference-year">(2019)</span>. Book Title. Publisher.
    </span>
  </div>

  <div class="reference">
    <span class="reference-type"><a id="dodici">[12]</a></span>
    <span class="reference-type">Book</span>
    <span class="reference-details">
      <span class="reference-author">Author Lastname, Firstname</span>,  
      <span class="reference-year">(2019)</span>. Book Title. Publisher.
    </span>
  </div>

  <!-- other references -->

</section>
</section> 
</body>
</html>